%!TEX root = ../../thesis.tex

\section{Previous Approaches: Feature-based Models}
\label{sec:feature-models}

% \red{TODO: What is the space of possible entities? How do you keep it from being too large?}

We first describe a strong feature-based model that we built in \newcite{chen2016thorough} for cloze style problems, in particular, the \sys{CNN/Daily Mail} dataset~\cite{hermann2015teaching}. We will then discuss similar models built for multiple choice and span prediction problems.

For the cloze style problems, the task is formulated as predicting the correct entity $a \in \mathcal{E}$ that can fill in the blank of the question $q$ based on reading the passage $p$ (one example can be found in Table~\ref{tab:rc-examples}), where $\mathcal{E}$ denotes the candidate set of entities. Conventional linear, feature-based classifiers usually need to construct a feature vector $f_{{p}, {q}}(e) \in \R^d$ for each candidate entity $e \in \mathcal{E}$, and to learn a weight vector $\mf{w} \in \R^d$ such that the correct answer $a$ is expected to rank higher than all other candidate entities:
\begin{equation}
\mf{w}^{\intercal}f_{p, q}(a) > \mf{w}^{\intercal}f_{{p}, {q}}(e), \forall e \in \mathcal{E} \setminus \{{a}\},
\end{equation}

After all the feature vectors are constructed for each entity $e$, we can then apply any popular machine learning algorithms (e.g., logistic regression or SVM). In \newcite{chen2016thorough}, we chose to use \sys{LambdaMART}~\cite{wu2010adapting}, as it is naturally a ranking problem and forests of boosted decision trees have been very successful lately.

\begin{table}[t]
\centering
\begin{tabular}{l p{14cm}}
\toprule
\tf{\#} & \tf{Feature} \\
\midrule
1 & Whether entity $e$ occurs in the passage. \\
2 & Whether entity $e$ occurs in the question. \\
3 & The \tf{frequency} of entity $e$ in the passage. \\
4 & The \tf{first position} of occurrence of entity $e$ in the passage. \\
5 & \tf{Word distance}: we align the placeholder with each occurrence of entity $e$, and compute the average minimum distance of each non-stop question word from the entity in the passage. \\
6 & \tf{Sentence co-occurrence}: whether entity $e$ co-occurs with another entity or verb that appears in the question, in some sentence of the passage. \\
7 & \tf{$n$-gram exact match}: whether there is an exact match between the text surrounding the placeholder and the text surrounding entity $e$. We have features for all combinations of matching left and/or right one or two words. \\
8 & \tf{Dependency parse match}: we dependency parse both the question and all the sentences in the passage, and extract an indicator feature of whether $w \xrightarrow{r} \text{@placeholder}$ and $w \xrightarrow{r} e$ are both found; similar features are constructed for $\text{@placeholder} \xrightarrow{r} w$ and $e \xrightarrow{r} w$. \\
\bottomrule
\end{tabular}
\longcaption{Features used in our entity-centric classifier}{\label{tab:classifier-features}Features used in our entity-centric classifier in \newcite{chen2016thorough}.}
\end{table}

The key question left is how can we build useful feature vectors from the passage $p$, the question $q$ and every entity $e$? Table~\ref{tab:classifier-features} lists 8 sets of features that we proposed for the \sys{CNN/Daily Mail} task. As shown in the table, these features are well designed and characterize the information of the entity (e.g., frequency, position and whether it is a question/passage word) and how they are aligned with the passage/question (e.g., co-occurrence, distance, linear and syntactic matching). Some features (\#6 and \#8) also rely on linguistic tools such as dependency parsing and part-of-speech tagging (deciding whether a word is a verb or not).  Generally speaking, for non-neural models, how to construct a useful set of features always remains as a challenge. Useful features need to be informational and well-tailored to specific tasks, while not too sparse to generalize well from the training set. We have argued before in Sec~\ref{sec:ml-approaches} that this is a common problem in most of the feature-based models. Also, using the off-the-shelf linguistic tools makes the model more expensive and their final performance depends on the the accuracy level of these annotations.

\newcite{rajpurkar2016squad} and \newcite{joshi2017triviaqa} also attempted to build feature-based models for the \sys{SQuAD} and \sys{TriviaQA} datasets respectively. The models are similar in spirit to ours, except that for these span prediction tasks, they need to first decide on a set of possible answers.
For \sys{SQuAD}, \newcite{rajpurkar2016squad} consider all constituents in parses generated by Stanford CoreNLP~\cite{manning2014stanford} as candidate answers; while for \sys{TriviaQA}, \newcite{joshi2017triviaqa} consider all $n$-gram ($1 \leq n \leq 5$) that occurs in the sentences which contain at least one word in common with the question. They also tried to add more lexicalized features and labels from constituency parses. Other attempts have been made for multiple choice problems such as \cite{wang2015machine} for the \sys{MCTest} dataset and a rich set of features have been used including semantic frames, word embeddings and coreference resolution.

We will demonstrate the empirical results of these feature-based classifiers and compare them to the neural models in Section~\ref{sec:sar-experiments}.
