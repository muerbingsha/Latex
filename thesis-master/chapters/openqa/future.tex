%!TEX root = ../../thesis.tex

\section{Future Work}
\label{sec:openqa-future}

Our \sys{DrQA} demonstrates that combining information retrieval and neural reading comprehension is an effective approach for open-domain question answering. We hope that our work takes the first step in this research direction. However, our system is still at an early stage and many implementation details can be further improved.

We think the following research directions will (greatly) improve our \sys{DrQA} system and should be pursued as future work. Indeed, some of the ideas have already been implemented in the following year after we published our \sys{DrQA} system and we will also describe them in detail in this section.

\paragraph{Aggregating evidence from multiple paragraphs.} Our system adopted the most simple and straightforward approach: we took the argmax over the unnormalized scores of all the retrieved passages. This is not ideal because 1) It implies that each passage must contain the correct answer (as \sys{SQuAD} examples) so our system will output one and only one answer for each passage. This is indeed not the case for most retrieved passages. 2) Our current training paradigm doesn't guarantee that the scores in different passages are comparable which causes a gap between the training and the evaluation process.

Training on full Wikipedia articles is a solution to alleviate this problem (see the \sys{DrQA*} results in Table~\ref{tab:drqa-full-results}), however, these models are running slowly and difficult to parallelize. \newcite{clark2018simple} proposed to perform multi-paragraph training with modified training objectives, where the span start and end scores are normalized across all paragraphs sampled from the same context. They demonstrated that it works much better than training on individual passages independently. Similarly, \newcite{wang2018r} and \newcite{wang2018evidence} proposed to train an explicit passage re-ranking component on the retrieved articles: \newcite{wang2018r} implemented this in a reinforcement learning framework so the re-ranker component and answer extraction components are jointly trained; \newcite{wang2018evidence} proposed a strength-based re-ranker and a coverage-based re-ranker which aggregate evidence from multiple paragraphs more directly.

\paragraph{Using more and better training data.} The second aspect which makes a big impact is the training data. Our \sys{DrQA} system only collected 44k distantly-supervised training examples from \sys{TREC}, \sys{WebQuestions} and \sys{WikiMovies}, and we demonstrated their effectiveness in Section~\ref{sec:drqa-final-results}. The system should be further improved if we can leverage more supervised training data --- from either \sys{TriviaQA}~\cite{joshi2017triviaqa} or generating more data from other QA resources. Moreover, these distantly supervised examples inevitably suffer from the noise problem (i.e., the paragraph doesn't imply the answer to the question even if the answer is contained) and \newcite{lin2018denoising} proposed a solution to de-noise these distantly supervised examples and demonstrated gains in an evaluation.

We also believe that adding negative examples should improve the performance of our system substantially. We can either create some negative examples using our full pipeline: we can leverage the \sys{Document Retrieval} module to help us find relevant paragraphs while they don't contain the correct answer. We can also incorporate existing resources such as \sys{SQuAD 2.0}~\cite{rajpurkar2018know} into our training process, which contains curated, high-quality negative examples.

\paragraph{Making the \sys{Document Retriever} trainable.} A third promising direction that has not been fully studied yet is to employ a machine learning approach for the \sys{Document Retriever} module. Our system adopted a straightforward, non-machine learning model and further improvement on the retrieval performance (Table~\ref{tab:ir-res}) should lead to an improvement on the full system. A training corpus for the \sys{Document Retriever} component can be collected either from other resources or from the QA data (e.g., using whether an article contains the answer to the question as a label). Joint training of the \sys{Document Retrieval} and the \sys{Document Reader} component will be a very desirable and promising direction for future work.

Related to this, \newcite{clark2018simple} also built an open-domain question answering system\footnote{The demo is at \href{https://documentqa.allenai.org}{https://documentqa.allenai.org}.} on top of a search engine (Bing web search) and demonstrated superior performance compared to ours. We think the results are not directly comparable and the two approaches (using a commercial search engine or building an independent IR component) both have pros and cons. Building our own IR component gets rid of an existing API call and can run faster and easily adapt to new domains.

\paragraph{Better \sys{Document Reader} module.} For our \sys{DrQA} system, we used the neural reading comprehension model which achieved F1 of 79.0\% on the test set of \sys{SQuAD 1.1}. With the recent development of neural reading comprehension models (Section~\ref{sec:advances}), we are confident that if we replace our current \sys{Document Reader} model with the state-of-the-art models~\cite{devlin2018bert}, the performance of our full system will be improved as well.

\paragraph{More analysis is needed.} Another important missing work is to conduct an in-depth analysis of our current systems: to understand which questions they can answer, and which they can't. We think it is important to compare our modern systems to the earlier TREC QA results under the same conditions. It will help us understand where we make genuine progress and what techniques we can still use from the pre-deep learning era, to build better question answering systems in the future.

Concurrent to our work, there are several works in a similar spirit to ours, including \sys{SearchQA}~\cite{dunn2017searchqa} and \sys{Quasar-T}~\cite{dhingra2017quasar}, which both collected relevant documents for trivia or \sys{Jeopardy!} questions --- the former one retrieved documents from \sys{ClueWeb} using the \sys{Lucene} index and the latter used \sys{Google} search. \sys{TriviaQA}~\cite{joshi2017triviaqa} also has an open-domain setting where all the retrieved documents from Bing web search are kept.
However, these datasets still focus on the task of question answering from the retrieved documents, while we are more interested in building an end-to-end QA system.
