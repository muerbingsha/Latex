%!TEX root = ../../thesis.tex

% \section{Introduction}
In \sys{Part I}, we described the task of reading comprehension: its formulation and development over recent years, the key components of neural reading comprehension systems, and future research directions. However, it is unclear yet whether reading comprehension is merely used as a task of measuring language understanding abilities, or it can enable any useful applications.  In \sys{Part II}, we will answer this question and discuss our efforts at building applications which leverage neural reading comprehension as their core component.

In this chapter, we view \ti{open domain question answering} as an application of reading comprehension. Open domain question answering has been a long-standing problem in the history of NLP\@. The goal of open domain question answering is to build automated computer systems which are able to answer any sort of (factoid) questions that humans might ask, based on a large collection of unstructured natural language documents, structured data (e.g., knowledge bases), semi-structured data (e.g., tables) or even other modalities such as images or videos.

We are the first to test how the neural reading comprehension methods can perform in an open-domain QA framework. We believe that the high performance of these systems can be a key ingredient in building a new generation of open-domain question answering systems, when combined with effective information retrieval techniques.

This chapter is organized as follows. We first give a high-level overview of open domain question answering and some notable systems in the history (Section~\ref{sec:openqa-rw}). Next, we introduce an open-domain question answering system that we built called \sys{DrQA}, designed to answer questions from English Wikipedia (Section~\ref{sec:drqa}). It essentially combines an information retrieval module and the high-performing neural reading comprehension module that we described in Section~\ref{sec:sar}. We further talk about how we can improve the system by creating distantly-supervised training examples from the retrieval module. We then present a comprehensive evaluation on multiple question answering benchmarks (Section~\ref{sec:drqa-eval}). Finally, we discuss current limitations, follow-up work and future directions in Section~\ref{sec:openqa-future}.
