%\documentclass[options]{class}
%\documentclass[12pt,UTF8,a4paper,twocolumn]{ctexart}
\documentclass[12pt,UTF8,a4paper,twocolumn]{article}

%----start preamble----
%commands that affect the entire document.
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage[left=10mm,right=10mm,top=10mm,bottom=20mm]{geometry} % A4 margins
%\usepackage{ctex} % chinese
\usepackage{graphicx} % image
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{color} % font color
\usepackage{multirow}
\usepackage{booktabs} % for table line
\renewcommand*{\Affilfont}{\small} % make departments small and italic
\renewcommand\Authands{, } % reove and before last author
\renewcommand{\abstractname}{Abstract} % revise abstract name







%----end preamble----


\begin{document}

  \title{\textbf{A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference}}



  \author[1]{\textbf{Adina Williams} \thanks{ adinawilliams@nyu.edu}}
  \author[2]{\textbf{Nikita Nangia} \thanks{niktanangia@nyu.edu}}
  \author[1,2,3]{\textbf{Samuel R. Bowman} \thanks{bowman@nyu.edu}}

  % \thanks is put in \author to indicate email which is put in footnote

  \author{Kim Deng \textsuperscript{}}


  \affil[1]{Department of Linguistics \authorcr
  New York University}
  \affil[2]{Center for Data Science \authorcr New York University}
  \affil[3]{Department of Computer Science \authorcr New York University}



  \date{} % remove date
  \maketitle % show title


%  \begin{center} % center abstract title
  \begin{abstract}
%  \end{center}
  This paper introduces the Multi-Genre Natural Language Inference (MutltiNLI) corpus, a dataset deisgned for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora avaiable fo natural language inference (a.k.a \emph{recognizing textual entailment}), imporving upon avaible resources in both its coverage and difficulty. MultiNLI accomplishes this by offering dafa from ten distinct genrs of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.
  \end{abstract}



  \section{Introduction}
  Many of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success. While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problmes, it must simultaneously succeed both at NLU, and at one or more additional hard marchine learning problems like structured prodiction or memory access. This makes it difficult to accurately juege the degree to which current models extract reasonable representations of language meaningn in these settings.
  \par % new line
  The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. In this task, also known as \emph{recognizing textual entailment} {\color{blue}(Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Market, 2005; Dagan et al., 2006; MacCartney and Manning, 2009)}, a model is presented with a pair of sentences----like one of those in Figure 1----and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, CONTRADICTION. Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representatios for the meanings of sentences (i.e., their lexical and compositional semantics). In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lixical and syntactic ambiguity.
  \par
  As the only large human-annotated corpus for NLI currently avaiable, the Stanford NLI Corpus (SNLI; {\color{blue}Bowman et al., 2015}) has enalbed a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention({\color{blue}Wang and Jiang, 2016; Parikh et al., 2016}), memory ({\color{blue}Munkhdalai and Yu, 2017}), and the use of parse stucture ({\color{blue}Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017}). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.
  \begin{table}
    \begin{tabular}{|mmm|}
      \hline
      rectangle & triangle & line \\
      \hline
      3 & 4 & 5 \\
      1 & 2 & 3 \\
      \hline
    \end{tabular}
    \qquad
    ($a^2 + b^2 = c^2$) % math equation
  \end{table}


  \begin{table}[!ht]
  \begin{tabular}{clrrrrrr}
  \toprule
  Capacity constraint
  &\multicolumn{6}{c}{Optimal solutions}
  \\
  \midrule
  \multirow{6}*{\shortstack{Cash constraint\\(Our model)}}
  &$x_{t}$  &1    &0    &1    &1    &0    &0\\
  &$y_{t}$  &20    &0    &77    &20    &100    &0  \\
  &$w_{t}$  &25    &18    &0    &0    &0    &0    \\
  &$Ed_t$ &30    &33    &41    &55    &45    &55    \\
  &$I_{t}$  &15    &0    &35    &0    &55    &0  \\
  &$B_{t}$  &0    &330    &648    &1177    &952    &1942    \\
  \bottomrule
  \end{tabular}
  \end{table}


  \begin{table*}
    \begin{tabular}{lll{3cm}}
      \toprule
      \multirow{3}*{Met my first girlfriend that way.}
      & FACE-TO-FACE & \multirow{3}*{I didn’t meet my first girlfriend until later.} \\
      ~ & \textbf{contradiction} & ~ \\
      ~ & CCNC & ~ \\
      \\
      \multirow{3}*{8 million in relief in the form of emergency housing.}
      & GOVERNMENT & \multirow{3}*{The 8 million dollars for emergency hous- ing was still not enough to solve the prob- lem.} \\
      ~ & \textbf{neutral} & ~ \\
      ~ & NNNN & ~ \\
      \\
      \multirow{3}*{8 million in relief in the form of emergency housing.}
      & GOVERNMENT & \multirow{3}*{The 8 million dollars for emergency hous- ing was still not enough to solve the prob- lem.} \\
      ~ & \textbf{neutral} & ~ \\
      ~ & NNNN & ~ \\
      \toprule






    \end{tabular}
  \end{table*}


  \par
  First, the sentences in SNLI are derived from only a single text genre----image captios----and are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomena----like temporal reasoning (e.g., \emph{yesterday}), bilief (e.g., \emph{know}), and modality {e.g., \emph{should}}----rare enough to be irrelevant to task performance. Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models.
  \par
  This paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English. While its size (433 pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.


  % @package mathtools
  \begin{equation}
      \prescript{14}{2}{\mathbf{C}}
  \end{equation}

  % @package amsmath
  \begin{equation}
      \sideset{_a^b}{^}\sum A_n
  \end{equation}

  \begin{equation}
    \ce{^{227}_{90}Th+}
  \end{equation}

  \begin{keywords}
    nlp
  \end{keywords}


  \section{一级标题}
  \subsection{二级标题}
  \subsubsection{三级标题}
  西游记\footnote{中国古典小说四大名著之一}小说开头说到
  \begin{quote}
    {\kaishu 东胜神洲有一花果山，山顶一石，受日月精华，生出一石猴。之}
  \end{quote}

\end{document}
