
\section{Introduction}
Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling \citep{Dzmitry2014}, sentiment classification, and spam detection. Traditional approaches of text classification represent documents with sparse lexical features, such as \emph{n}-grams, and then use a linear model or kernel methods on this representation. More recent approaches used deep learning , such as convolutional neural networks and recurrent neural networks based on long short-term memory (LSTM) to learn text representations. 

\begin{figure} [h]
\caption{A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word \emph{delicious, a-m-a-z-i-n-g} contributes the most in defining sentiment of the two sentences.}\label{fig1}

Although neural-network-based approaches to text classification have been quite effective, in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation.

Our primary contribution is a new neural architecture(2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents have a hierarchical structure (words form sentences, sentences form a document), we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. Moreover, the importance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context \ref{sec3.5}. To include sensitivity to this fact, our model includes two levels of attention mechanisms ---- one at the word level and one  at the sentence level ---- that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. To illustrate, consider the example in Fig. 1, which is a short Yelp review where the task is to predict the rating on a scale from 1-5. Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word \emph{delicious}, \emph{a-m-a-z-i-n-g} contributes more in implying the positive attitude contained in this review. Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis.

The key difference to previous work is that our system uses  \emph{context} to discover \emph{when} a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets \ref{sec3). Our model outperforms previous approaches by a significant margin.

\end{figure} 




